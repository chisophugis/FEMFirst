p1 and p2 contain the beginnings of some derivations, with mistakes.

p3 contains a mostly complete/correct derivation.
p4 contains the reduction of the stuff from p3 to the algorithm to be
implemented.

The key idea is the following:
You start with a guess at a vector x that solves Ax = b.
(A and b are givens; A is a  matrix[1]; b is
a vector)
This guess is wrong, say the correct solution is x*.
If you had access to the quantity e = x - x*, known as "the error",
then you would be able to solve the problem immediately.
But you don't have access.
So instead work with a quantity r = -Ae = A(x* - x) = b - Ax, known as
"the residual". Since this quantity is just `b - Ax` you can compute
it.
So the trick is to do operations on `e` without being able to
explicitly manipulate it. The goal is to make `e` go to zero.
One way of doing that is iterate:
e_(i+1) = e_(i) - "projection of e_(i) onto a new linearly independent
direction d_(i)"
The trick for doing this without explicitly manipulating `e` is to do
the projection in the norm induced by A (A is symmetric
positive-definite so it induces a norm).
In the A-norm, an inner product expression like <d,e>_A ("inner
product of d and e in the A norm") is `d^T A e = -d^T r`.
Now the expression is in terms of `r`, which we can explicitly
compute!

In general, when r is orthogonal to something, e is A-orthogonal to
that thing because d^T r = -d^T A e.

Shewchuk's "An Introduction to the Conjugate Gradient Method Without
the Agonizing Pain" is really good and I recommend reading it.

[1] It must be symmetric positive-definite, which essentially means
that x^T A x is a multidimensional parabola with a unique minimum.
E.g. if A is an identity matrix, then
x^T A x = (x_1)^2 + (x_2)^2 + ... + (x_N)^2
The matrices generated by a roller-spring system are symmetric
(essentially a statement of Newton's third law)
and positive-definite (essentially saying that all the spring
constants are positive).
